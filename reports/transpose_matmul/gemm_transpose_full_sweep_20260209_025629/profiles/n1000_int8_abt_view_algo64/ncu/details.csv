"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description","Estimated Speedup Type","Estimated Speedup"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","DRAM Frequency","Ghz","3.99",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","SM Frequency","Ghz","1.13",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","57528",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","Memory Throughput","%","21.70",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.61",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","Duration","us","49.95",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","59.24",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","5.03",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20773.66",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.49",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","SpeedOfLight","","","","SOLBottleneck","OPT","This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full waves across all SMs. Look at Launch Statistics for more details.","",""
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Block Size","","256",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Cluster Scheduling Policy","","PolicySpread",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Cluster Size","","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Grid Size","","64",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Preferred Cluster Size","","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Registers Per Thread","register/thread","122",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","135.17",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Driver Shared Memory Per Block","Kbyte/block","1.02",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Dynamic Shared Memory Per Block","Kbyte/block","34.82",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","# SMs","SM","148",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Stack Size","","1024",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Threads","thread","16384",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","# TPCs","","74",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Enabled TPC IDs","","all",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Uses Green Context","","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Launch Statistics","Waves Per SM","","0.22",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","LaunchStats","","","","LaunchConfiguration","OPT","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 148 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations.","global","56.76"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Max Active Clusters","cluster","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Max Cluster Size","block","8",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Overall GPU Occupancy","%","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Cluster Occupancy","%","0",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Block Limit Barriers","block","32",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Block Limit SM","block","32",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Block Limit Registers","block","2",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Block Limit Shared Mem","block","3",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Block Limit Warps","block","8",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Theoretical Active Warps per SM","warp","16",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Theoretical Occupancy","%","25",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Achieved Occupancy","%","12.50",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","Achieved Active Warps Per SM","warp","8.00",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","","","","AchievedOccupancy","OPT","The difference between calculated theoretical (25.0%) and measured achieved occupancy (12.5%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy.","local","49.98"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","Occupancy","","","","TheoreticalOccupancy","OPT","The 4.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the hardware maximum of 16. This kernel's theoretical occupancy (25.0%) is limited by the number of required registers.","local","75"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Average DRAM Active Cycles","cycle","991.12",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Total DRAM Elapsed Cycles","cycle","12753920",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Average L1 Active Cycles","cycle","20773.66",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Total L1 Elapsed Cycles","cycle","8392140",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Average L2 Active Cycles","cycle","42246.40",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Total L2 Elapsed Cycles","cycle","10603184",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Average SM Active Cycles","cycle","20773.66",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Total SM Elapsed Cycles","cycle","8392140",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Average SMSP Active Cycles","cycle","20929.59",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","GPU and Memory Workload Distribution","Total SMSP Elapsed Cycles","cycle","33568560",
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 58.58% above the average, while the minimum instance value is 100.00% below the average.","global","21.46"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 58.77% above the average, while the minimum instance value is 100.00% below the average.","global","21.69"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L1 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 58.58% above the average, while the minimum instance value is 100.00% below the average.","global","21.46"
"0","887148","repro_algo23_int8_n1000","127.0.0.1","void Kernel2<cutlass_80_wmma_tensorop_i161616gemm_s8_forwardCompat_128x128_32x2_tn_align4>(Params)","1","20","(256, 1, 1)","(64, 1, 1)","0","10.0","WorkloadDistribution","","","","WorkloadImbalance","OPT","One or more L2 Slices have a much lower number of active cycles than the average number of active cycles. Maximum instance value is 7.52% above the average, while the minimum instance value is 16.51% below the average.","global","5.516"